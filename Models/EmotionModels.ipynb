{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "EmotionModels.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl9Ht2uggCQw"
      },
      "source": [
        "# **Requried Downloads**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNdlEs3mgCQz"
      },
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwMqdpb2gCQ2"
      },
      "source": [
        "# **Required Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adjxCh2igCQ3"
      },
      "source": [
        "\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from transformers import RobertaTokenizer,TFRobertaForSequenceClassification\n",
        "from transformers import XLNetTokenizer, TFXLNetForSequenceClassification\n",
        "from transformers import ElectraTokenizer,TFElectraForSequenceClassification\n",
        "from transformers import GPT2Tokenizer,TFGPT2ForSequenceClassification\n",
        "from transformers import FlaubertTokenizer,TFFlaubertForSequenceClassification\n",
        "from transformers import DebertaTokenizer,TFDebertaForSequenceClassification\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47pKS2lxhB3C"
      },
      "source": [
        "# **Data Prepration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz-Rx6FJokN-"
      },
      "source": [
        "## Importing Data\n",
        "- The following cell is importing data from google drive and storing the files to /home/Emotiondata directory.\n",
        "- Data Source: \n",
        "> https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAEtK1AGhGD7",
        "outputId": "ecba6b77-6f3e-4d9f-f507-58eadba55cae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szJKMtjzqqah",
        "outputId": "33048f9b-8f0f-432e-b45d-797a5c3674cb"
      },
      "source": [
        "DataDir = '/home/EmotionData'\n",
        "!mkdir /home/EmotionData\n",
        "!unzip /content/drive/MyDrive/archive.zip \n",
        "!mv /content/*.txt /home/EmotionData"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/archive.zip\n",
            "  inflating: test.txt                \n",
            "  inflating: train.txt               \n",
            "  inflating: val.txt                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx8Kr2o-sWO-"
      },
      "source": [
        "## Loading Data\n",
        "- The Following cell loads train, validation and test data to variables \n",
        "- Shows initially how the data looks\n",
        "- Used Sources: \n",
        ">https://stackoverflow.com/questions/21546739/load-data-from-txt-with-pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZedJaw-rsZ16",
        "outputId": "c842489a-11d8-4e63-fb27-3b484fb89af7"
      },
      "source": [
        "trainData = pd.read_csv(DataDir+'/train.txt', sep=\";\", header=None, names = ['Text','Label'])\n",
        "valData = pd.read_csv(DataDir+'/val.txt', sep=\";\", header=None, names = ['Text','Label'])\n",
        "testData = pd.read_csv(DataDir+'/test.txt', sep=\";\", header=None, names = ['Text','Label'])\n",
        "trainData.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i didnt feel humiliated</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i can go from feeling so hopeless to so damned...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i am feeling grouchy</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text    Label\n",
              "0                            i didnt feel humiliated  sadness\n",
              "1  i can go from feeling so hopeless to so damned...  sadness\n",
              "2   im grabbing a minute to post i feel greedy wrong    anger\n",
              "3  i am ever feeling nostalgic about the fireplac...     love\n",
              "4                               i am feeling grouchy    anger"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}